
# 大数据场景问题
大数据：数据量很大，**无法一次处理，需要分批存储、处理**；

1、针对内存：分治；
- 分批处理
- 按照规则同时分到不同的节点处理；

2、针对时间效率：采用高效的数据结构；
- bitmap：随机访问；高效去重；
- hashmap：
- Trie：前缀模糊匹配；

# 大数据去重

优先内存：每次只能加载一定量的数据；要做全量的去重，需要将原始数据映射成另一种方式去重；

## bitmap

将数据通过哈希算法，映射成一个小于固定值的数字；

通过bitmap索引，来进行去重；

- java中使用byte数组，10亿的bitmap大概占用不到2G内存；
- Java的HashCode方法，结果为32位有符号整数；

## 分治 + 中间持久化

将数据哈希计算后，存储到特定的文件；(文件的数量可以调控)

定位到此文件，在当前文件内去重，已经存在则忽略，不存在则存入；

缺点：IO太多；效率会比较低；

优点：
- 可以中断继续，因为持久化，不会影响；
- 不受哈希冲突影响，因为最终判重，是通过比对原始数据；
